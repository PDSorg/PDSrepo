{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A transformer-based unified multimodal framework for Alzheimer's disease assessment\n",
    "\n",
    "<https://www.sciencedirect.com/science/article/pii/S0010482524010643#bib38>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **model interpretation**\n",
    "\n",
    "Applying **Grad-CAM** \n",
    "\n",
    "> The gradient value of each token offers a cue for interpreting the models decisions, illuminating the specific tokens or positions within the input image and non-image data that influenced the models output.\n",
    ">\n",
    "> \n",
    "> Through assigning SHAP values to specific voxels or by mapping internal network nodes, SHAP offered insights into the contribution of individual features to the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, embed_dim=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, E, H/P, W/P]\n",
    "        x = x.flatten(2)  # [B, E, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, E]\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        num_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_dim=3072,\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size=img_size, patch_size=patch_size, embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, (img_size // patch_size) ** 2 + 1, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=embed_dim,\n",
    "                    nhead=num_heads,\n",
    "                    dim_feedforward=mlp_dim,\n",
    "                    dropout=dropout_rate,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, _, _ = x.size()\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = VisionTransformer(img_size=224, patch_size=16, num_classes=1000)\n",
    "\n",
    "# Fake data for testing\n",
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Should print torch.Size([1, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, embed_dim=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B: batch size, C: channels, H: height, W: width, N: num_patches\n",
    "        x = self.conv(x)  # [B, C, H/P, W/P]\n",
    "        x = x.flatten(2)  # [B, C, N]\n",
    "        x = x.transpose(1, 2)  # [B, N, C]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# check shape of output\n",
    "img_size, patch_size = 224, 16\n",
    "pe = PatchEmbedding(img_size, patch_size)\n",
    "x = torch.randn(1, 3, img_size, img_size)\n",
    "print(pe(x).shape)  # torch.Size([1, 196, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.size()\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.fc(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of output\n",
    "pe = PatchEmbedding(224, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_dim, dropout=0.1):\n",
    "        super(ViTMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        # print(\"ViTMLP shape:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.mlp = ViTMLP(embed_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        in_channels,\n",
    "        num_classes,\n",
    "        embed_dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        mlp_dim,\n",
    "        blk_dropout=0.1,\n",
    "        emb_dropout=0.1,\n",
    "        lr=1e-4,\n",
    "    ):\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, self.patch_embed.num_patches + 1, embed_dim)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [ViTBlock(embed_dim, num_heads, mlp_dim, blk_dropout) for _ in range(depth)]\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, data, num_epochs=10, learning_rate=1e-4):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, batch in enumerate(self.data):\n",
    "                inputs, labels = batch\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            avg_loss = total_loss / len(self.data)\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f'Epoch [{epoch + 1}/{self.num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "ViTMLP shape: torch.Size([1, 197, 768])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# check shape of output\n",
    "img_size, patch_size, in_channels = 224, 16, 3\n",
    "num_classes, embed_dim, depth, num_heads, mlp_dim = 1000, 768, 12, 12, 3072\n",
    "vit = ViT(\n",
    "    img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim\n",
    ")\n",
    "x = torch.randn(1, in_channels, img_size, img_size)\n",
    "print(vit(x).shape)  # torch.Size([1, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10930/326672583.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  data = torch.tensor(data).float()\n"
     ]
    }
   ],
   "source": [
    "# load temp data\n",
    "import nibabel as nib\n",
    "\n",
    "nii_file = ['../temp/temp/I35933.nii',\n",
    "            '../temp/temp/I50468.nii',\n",
    "            '../temp/temp/I64631.nii']\n",
    "data = [nib.load(nii_file[i]).get_fdata() for i in range(len(nii_file))]\n",
    "data = torch.tensor(data).float()\n",
    "data = data.permute(0, 3, 1, 2)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 160, 192, 192])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data\n",
    "labels = torch.tensor([0, 1, 2])\n",
    "data_batch = (inputs, labels)\n",
    "data_batch = [data_batch for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/10], Loss: 1.7117\n",
      "Epoch [2/10], Step [10/10], Loss: 1.1826\n",
      "Epoch [3/10], Step [10/10], Loss: 0.9688\n",
      "Epoch [4/10], Step [10/10], Loss: 0.6730\n",
      "Epoch [5/10], Step [10/10], Loss: 0.0609\n",
      "Epoch [6/10], Step [10/10], Loss: 0.0019\n",
      "Epoch [7/10], Step [10/10], Loss: 0.0001\n",
      "Epoch [8/10], Step [10/10], Loss: 0.0001\n",
      "Epoch [9/10], Step [10/10], Loss: 0.0001\n",
      "Epoch [10/10], Step [10/10], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "img_size, patch_size, in_channels = 192, 16, 160\n",
    "num_classes, embed_dim, depth, num_heads, mlp_dim = 3, 768, 12, 12, 3072\n",
    "vit = ViT(\n",
    "    img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim\n",
    ")\n",
    "trainer = Trainer(vit, data_batch)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
